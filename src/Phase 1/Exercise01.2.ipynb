{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Exercise 1.2: Implement Gradient Descent for a Single Neuron\n",
    "\n",
    "üí° Goal: Adjust w and b to minimize error using Mean Squared Error (MSE) loss.\n",
    "\n",
    "## üîπ Steps:\n",
    "\n",
    "- Define a loss function:\n",
    "\n",
    "    Loss = ( 1 / ùëõ ) ‚àë( ùë¶pred ‚àí ùë¶true )^2\n",
    "\n",
    "- Compute gradients of `w` and `b`\n",
    "- Update `w` and `b` using **Gradient Descent**\n",
    "- Train the neuron on some **sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0288\n",
      "Epoch 100, Loss: 0.1369\n",
      "Epoch 200, Loss: 0.0808\n",
      "Epoch 300, Loss: 0.0562\n",
      "Epoch 400, Loss: 0.0425\n",
      "Epoch 500, Loss: 0.0339\n",
      "Epoch 600, Loss: 0.0281\n",
      "Epoch 700, Loss: 0.0239\n",
      "Epoch 800, Loss: 0.0207\n",
      "Epoch 900, Loss: 0.0182\n",
      "Final weight: 4.6467938623597895\n",
      "Final bias: 2.205176879018755\n",
      "Final Neuron Outputs: [0.0008340266344459207, 0.0800537483072916, 0.9007134340146822, 0.9989437470142934, 0.9999898574868596]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some sample training data\n",
    "X_train = np.array([-2, -1, 0, 1, 2])  # Inputs\n",
    "y_train = np.array([0, 0, 1, 1, 1])    # Desired outputs (labels)\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "learning_rate = 0.1\n",
    "epochs = 1000  # Number of training iterations\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        x = X_train[i]\n",
    "        y_true = y_train[i]\n",
    "\n",
    "        # Forward pass\n",
    "        z = w * x + b\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # Compute loss (MSE)\n",
    "        loss = (y_pred - y_true) ** 2\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backpropagation: Compute gradients\n",
    "        dL_dy = 2 * (y_pred - y_true)  # dL/dy_pred\n",
    "        dy_dz = sigmoid_derivative(z)  # dy_pred/dz\n",
    "        dz_dw = x  # dz/dw\n",
    "        dz_db = 1  # dz/db\n",
    "\n",
    "        # Compute gradients\n",
    "        dL_dw = dL_dy * dy_dz * dz_dw\n",
    "        dL_db = dL_dy * dy_dz * dz_db\n",
    "\n",
    "        # Update parameters\n",
    "        w -= learning_rate * dL_dw\n",
    "        b -= learning_rate * dL_db\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Final trained values\n",
    "print(\"Final weight:\", w)\n",
    "print(\"Final bias:\", b)\n",
    "\n",
    "# Test the trained neuron\n",
    "output = [sigmoid(w * x + b) for x in X_train]\n",
    "print(\"Final Neuron Outputs:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Analysis of the Output\n",
    "\n",
    "‚úÖ Loss Decreasing: The loss started at **1.0911** and reduced to **0.0176**, showing that the neuron optimized itself over epochs.\n",
    "\n",
    "‚úÖ **Final Weight** (`w`) and **Bias** (`b`):\n",
    "\n",
    "- `w = 4.68`: This means the neuron gives a **strong weight** to the input.\n",
    "- `b = 2.22`: A positive bias shifts the output towards **1**. ‚úÖ Final Outputs:\n",
    "\n",
    "- **Low values (-2, -1) ‚Üí Close to 0** ‚úÖ\n",
    "- **High values (0, 1, 2) ‚Üí Close to 1** ‚úÖ\n",
    "- The neuron correctly classifies the input data **as expected!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
