{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Exercise 2: Implement a Simple 3-Layer Neural Network\n",
    "\n",
    "üí° **Goal:** Implement a neural network with\n",
    "\n",
    "- ‚úîÔ∏è **3 layers**: Input ‚Üí Hidden ‚Üí Output\n",
    "- ‚úîÔ∏è **Activation function**: Sigmoid\n",
    "- ‚úîÔ∏è **Backpropagation**: Train using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2751\n",
      "Epoch 100, Loss: 0.1410\n",
      "Epoch 200, Loss: 0.0483\n",
      "Epoch 300, Loss: 0.0210\n",
      "Epoch 400, Loss: 0.0120\n",
      "Epoch 500, Loss: 0.0080\n",
      "Epoch 600, Loss: 0.0059\n",
      "Epoch 700, Loss: 0.0046\n",
      "Epoch 800, Loss: 0.0037\n",
      "Epoch 900, Loss: 0.0031\n",
      "\n",
      "Final Predictions:\n",
      "[[0.02766161]\n",
      " [0.07528557]\n",
      " [0.92075152]\n",
      " [0.98371912]\n",
      " [0.98572941]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    x (float or ndarray): Input value(s).\n",
    "\n",
    "    Returns:\n",
    "    float or ndarray: Sigmoid activation result.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Step 2: Define the derivative of the Sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (float or ndarray): Input value(s), assuming it's already a sigmoid output.\n",
    "\n",
    "    Returns:\n",
    "    float or ndarray: Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    return x * (1 - x)  # Sigmoid' = sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Step 3: Define Training Data (inputs and expected outputs)\n",
    "X = np.array([[-2], [-1], [0], [1], [2]])  # Input values (features)\n",
    "y = np.array([[0], [0], [1], [1], [1]])    # Expected outputs (labels)\n",
    "\n",
    "# Step 4: Define the Neural Network Architecture\n",
    "input_size = 1   # Number of input neurons\n",
    "hidden_size = 2  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Number of output neurons\n",
    "\n",
    "# Step 5: Initialize Weights and Biases\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights from input to hidden layer\n",
    "b1 = np.random.randn(hidden_size)              # Biases for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size) # Weights from hidden to output layer\n",
    "b2 = np.random.randn(output_size)              # Bias for output layer\n",
    "\n",
    "# Step 6: Define Hyperparameters\n",
    "learning_rate = 0.1  # Learning rate for weight updates\n",
    "epochs = 1000        # Number of iterations for training\n",
    "\n",
    "# Step 7: Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    Z1 = np.dot(X, W1) + b1  # Linear transformation: Input ‚Üí Hidden layer\n",
    "    A1 = sigmoid(Z1)         # Apply activation function to hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2 # Linear transformation: Hidden ‚Üí Output layer\n",
    "    A2 = sigmoid(Z2)         # Apply activation function to output layer (final prediction)\n",
    "\n",
    "    # Compute Loss (Mean Squared Error)\n",
    "    loss = np.mean((A2 - y) ** 2)  # Calculate the average squared error\n",
    "\n",
    "    # Backpropagation - Compute Gradients\n",
    "    dL_dA2 = 2 * (A2 - y)  # Derivative of loss w.r.t A2 (output)\n",
    "    dA2_dZ2 = sigmoid_derivative(A2)  # Derivative of sigmoid output w.r.t Z2\n",
    "    dZ2_dW2 = A1  # Gradient of Z2 w.r.t W2 (Hidden activations)\n",
    "    dZ2_dB2 = 1   # Gradient of Z2 w.r.t b2 (Bias derivative)\n",
    "\n",
    "    # Compute gradients for output layer parameters\n",
    "    dL_dW2 = np.dot(dZ2_dW2.T, dL_dA2 * dA2_dZ2)  # Weight gradient for W2\n",
    "    dL_dB2 = np.sum(dL_dA2 * dA2_dZ2, axis=0)     # Bias gradient for b2\n",
    "\n",
    "    # Compute gradients for hidden layer parameters\n",
    "    dZ2_dA1 = W2  # Gradient of Z2 w.r.t A1\n",
    "    dA1_dZ1 = sigmoid_derivative(A1)  # Gradient of hidden activation w.r.t Z1\n",
    "    dL_dW1 = np.dot(X.T, np.dot(dL_dA2 * dA2_dZ2, dZ2_dA1.T) * dA1_dZ1)  # Weight gradient for W1\n",
    "    dL_dB1 = np.sum(np.dot(dL_dA2 * dA2_dZ2, dZ2_dA1.T) * dA1_dZ1, axis=0)  # Bias gradient for b1\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    W2 -= learning_rate * dL_dW2  # Update output layer weights\n",
    "    b2 -= learning_rate * dL_dB2  # Update output layer bias\n",
    "    W1 -= learning_rate * dL_dW1  # Update hidden layer weights\n",
    "    b1 -= learning_rate * dL_dB1  # Update hidden layer bias\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Step 8: Final Output Predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "output = sigmoid(np.dot(sigmoid(np.dot(X, W1) + b1), W2) + b2)  # Compute final outputs\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Analysis of Your Output\n",
    "\n",
    "‚úÖ Loss Reduction:\n",
    "\n",
    "- Epoch 0: 0.2751\n",
    "- Epoch 900: 0.0031 (almost zero, meaning the network is highly accurate) ‚úÖ Final Predictions:\n",
    "- For input -2 ‚Üí 0.0276 (almost 0, ‚úÖ correct)\n",
    "- For input -1 ‚Üí 0.0752 (closer to 0, ‚úÖ correct)\n",
    "- For input 0 ‚Üí 0.9207 (closer to 1, ‚úÖ correct)\n",
    "- For input 1 ‚Üí 0.9837 (almost 1, ‚úÖ correct)\n",
    "- For input 2 ‚Üí 0.9857 (almost 1, ‚úÖ correct)\n",
    "\n",
    "üöÄ Your network is successfully classifying the inputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
