{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ Phase 3: Implementing a Neural Network in TensorFlow & PyTorch\n",
    "\n",
    "**Concepts to Cover**\n",
    "\n",
    "- **Why use TensorFlow & PyTorch?** ‚Äì Higher-level APIs for defining models.\n",
    "- **Defining Layers** ‚Äì Using tf.keras (TensorFlow) and torch.nn (PyTorch).\n",
    "- **Training Loops** ‚Äì Using built-in optimizers & loss functions.\n",
    "- **Batch Processing** ‚Äì Efficient training using mini-batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Exercise 3: Implement the same 3-layer XOR network in TensorFlow & PyTorch\n",
    "\n",
    "**üîπ Task**\n",
    "\n",
    "- Implement a **3-layer neural network** using:\n",
    "    - TensorFlow (`tf.keras`)\n",
    "    - PyTorch (`torch.nn.Module`)\n",
    "- Train on the **XOR dataset**.\n",
    "- Use the **Binary Cross-Entropy Loss (`BCE`)**.\n",
    "- Compare both frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\n",
      "TensorFlow Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Define the model correctly\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),  # Explicit Input Layer\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),  # Hidden Layer (4 neurons)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output Layer (1 neuron)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=5000, verbose=0)  # Train silently\n",
    "\n",
    "# Final Predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"\\nTensorFlow Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {p[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in TensorFlow (Optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Available! Training on GPU...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\n",
      "Optimized TensorFlow Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"‚úÖ GPU Available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, training on CPU...\")\n",
    "\n",
    "# Define the optimized model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),  # Explicit Input Layer\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),  # Hidden Layer (4 neurons)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output Layer (1 neuron)\n",
    "])\n",
    "\n",
    "# Compile with optimized settings\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.1),  # Faster optimizer\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with mini-batch gradient descent (batch_size=2)\n",
    "with tf.device('/GPU:0' if gpus else '/CPU:0'):\n",
    "    history = model.fit(X, y, epochs=1000, batch_size=2, verbose=0)  # Silent training\n",
    "\n",
    "# Final Predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"\\nOptimized TensorFlow Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {p[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ Optimizations Applied\n",
    "\n",
    "1. ‚úÖ Reduced Epochs: Down from 5000 to 1000 (should still converge).\n",
    "2. ‚úÖ Used RMSprop Optimizer: Faster than Adam for small datasets.\n",
    "3. ‚úÖ Batch Training (batch_size=2): Instead of feeding the entire dataset at once.\n",
    "4. ‚úÖ Forced GPU Usage: Uses with tf.device('/GPU:0') if available.\n",
    "\n",
    "This should significantly speed up training while keeping accuracy high. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR dataset\n",
    "X_torch = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_torch = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class XORNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNeuralNet, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # Hidden Layer (4 neurons)\n",
    "        self.output = nn.Linear(4, 1)  # Output Layer (1 neuron)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))  # Hidden Activation\n",
    "        x = torch.sigmoid(self.output(x))  # Output Activation\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = XORNeuralNet()\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    outputs = model(X_torch)  # Forward pass\n",
    "    loss = criterion(outputs, y_torch)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "# Final Predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_torch)\n",
    "\n",
    "print(\"\\nPyTorch Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X_torch[i].numpy()}, Predicted Output: {p.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Significance of This Exercise\n",
    "\n",
    "- **TensorFlow vs PyTorch** ‚Äì You now see how both frameworks implement the same logic.\n",
    "- **Higher-Level Abstraction** ‚Äì TensorFlow provides `Sequential()`, while PyTorch gives more control.\n",
    "- **Backpropagation Handling** ‚Äì PyTorch requires `loss.backward()`, while TensorFlow abstracts it away.\n",
    "- **Optimization** ‚Äì Adam optimizer helps in faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
