{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Phase 1: Single Neuron Implementation\n",
    "\n",
    "**Concepts to Cover**\n",
    "\n",
    "- **Perceptron Model** â€“ Understanding the fundamental unit of a neural network.\n",
    "- **Activation Functions** â€“ Step, Sigmoid, ReLU, and their importance.\n",
    "- **Forward Propagation** â€“ Calculating weighted sum and applying activation function.\n",
    "- **Gradient Descent** â€“ How a neuron learns by updating weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implement a Single Neuron\n",
    "\n",
    "ðŸ”¹  **Task**\n",
    "\n",
    "- Implement a single neuron (perceptron) in Python using only NumPy.\n",
    "- The neuron should take two inputs and have a trainable weight and bias.\n",
    "- Use the sigmoid activation function.\n",
    "- Implement forward propagation and gradient descent to adjust weights.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Initialize **random weights** and **bias**.\n",
    "- Compute the **weighted sum** of inputs.\n",
    "    Z = W1 . X1 + W2 . X2 + b\n",
    "- Apply the **sigmoid activation function**.\n",
    "    A = 1 / 1 + e^-z\n",
    "- Calculate the loss (Mean Squared Error or **Binary Cross-Entropy**.\n",
    "- Perform **gradient descent** to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9452\n",
      "Epoch 500, Loss: 0.2621\n",
      "Epoch 1000, Loss: 0.1799\n",
      "Epoch 1500, Loss: 0.1417\n",
      "Epoch 2000, Loss: 0.1191\n",
      "Epoch 2500, Loss: 0.1040\n",
      "Epoch 3000, Loss: 0.0932\n",
      "Epoch 3500, Loss: 0.0849\n",
      "Epoch 4000, Loss: 0.0784\n",
      "Epoch 4500, Loss: 0.0731\n",
      "\n",
      "Final Predictions:\n",
      "Input: [0 0], Predicted Output: 0.0009\n",
      "Input: [0 1], Predicted Output: 0.0819\n",
      "Input: [1 0], Predicted Output: 0.0819\n",
      "Input: [1 1], Predicted Output: 0.9022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Training data (AND logic gate)\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1], \n",
    "              [1, 0], \n",
    "              [1, 1]])  # Inputs\n",
    "\n",
    "y = np.array([[0], [0], [0], [1]])  # AND Output\n",
    "\n",
    "# Initialize weights and bias randomly\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(2, 1)\n",
    "bias = np.random.randn(1)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "epochs = 5000  # Number of iterations\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    weighted_sum = np.dot(X, weights) + bias\n",
    "    output = sigmoid(weighted_sum)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y, output)\n",
    "    \n",
    "    # Backpropagation (Gradient Descent)\n",
    "    error = output - y  # Difference between predicted and actual\n",
    "    d_output = error * sigmoid_derivative(output)  # Gradient of loss w.r.t output\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= lr * np.dot(X.T, d_output)\n",
    "    bias -= lr * np.sum(d_output)\n",
    "\n",
    "    # Print loss every 500 epochs\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for i, x in enumerate(X):\n",
    "    pred = sigmoid(np.dot(x, weights) + bias)\n",
    "    print(f\"Input: {x}, Predicted Output: {pred[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Significance of This Exercise\n",
    "\n",
    "1. **Understanding a Single Neuron** â€“ This is the fundamental building block of deep learning.\n",
    "2. **Activation Functions** â€“ Using sigmoid to make predictions between 0 and 1.\n",
    "3. **Forward Propagation** â€“ Computing the weighted sum and activation.\n",
    "4. **Loss Function** â€“ Binary Cross-Entropy measures how well the neuron performs.\n",
    "5. **Gradient Descent** â€“ Updating weights using derivatives to minimize loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
