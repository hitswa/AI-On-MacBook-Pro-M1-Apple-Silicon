{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Exercise 1.2: Implement Gradient Descent for a Single Neuron\n",
    "\n",
    "üí° Goal: Adjust w and b to minimize error using Mean Squared Error (MSE) loss.\n",
    "\n",
    "## üîπ Steps:\n",
    "\n",
    "- Define a loss function:\n",
    "\n",
    "    Loss = ( 1 / ùëõ ) ‚àë( ùë¶pred ‚àí ùë¶true )^2\n",
    "\n",
    "- Compute gradients of `w` and `b`\n",
    "- Update `w` and `b` using **Gradient Descent**\n",
    "- Train the neuron on some **sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4978\n",
      "Epoch 100, Loss: 0.1393\n",
      "Epoch 200, Loss: 0.0817\n",
      "Epoch 300, Loss: 0.0566\n",
      "Epoch 400, Loss: 0.0428\n",
      "Epoch 500, Loss: 0.0341\n",
      "Epoch 600, Loss: 0.0282\n",
      "Epoch 700, Loss: 0.0240\n",
      "Epoch 800, Loss: 0.0208\n",
      "Epoch 900, Loss: 0.0183\n",
      "Final weight: 4.643631924726756\n",
      "Final bias: 2.203567813349657\n",
      "Final Neuron Outputs: [0.0008379648928901648, 0.0801681843926923, 0.9005694444835305, 0.9989387009482799, 0.9999897767077403]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate some sample training data\n",
    "X_train = np.array([-2, -1, 0, 1, 2])  # Inputs (features)\n",
    "y_train = np.array([0, 0, 1, 1, 1])    # Corresponding labels (desired outputs)\n",
    "\n",
    "# Step 2: Initialize parameters (weights and bias)\n",
    "w = np.random.randn()  # Randomly initialize the weight\n",
    "b = np.random.randn()  # Randomly initialize the bias\n",
    "learning_rate = 0.1    # Learning rate for gradient descent\n",
    "epochs = 1000          # Number of training iterations\n",
    "\n",
    "# Step 3: Define the Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The input value.\n",
    "\n",
    "    Returns:\n",
    "    float: The sigmoid output (a value between 0 and 1).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Step 4: Compute the derivative of the Sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The input value.\n",
    "\n",
    "    Returns:\n",
    "    float: The derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Step 5: Training loop using gradient descent\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0  # Track the total loss for this epoch\n",
    "\n",
    "    for i in range(len(X_train)):  # Iterate over each training sample\n",
    "        x = X_train[i]      # Get the input\n",
    "        y_true = y_train[i] # Get the corresponding true label\n",
    "\n",
    "        # Step 6: Forward pass (calculate neuron output)\n",
    "        z = w * x + b       # Linear combination of inputs\n",
    "        y_pred = sigmoid(z) # Apply sigmoid activation\n",
    "\n",
    "        # Step 7: Compute Mean Squared Error (MSE) loss\n",
    "        loss = (y_pred - y_true) ** 2  # Squared error loss\n",
    "        total_loss += loss  # Accumulate loss for this epoch\n",
    "\n",
    "        # Step 8: Backpropagation - Compute gradients\n",
    "        dL_dy = 2 * (y_pred - y_true)  # Derivative of loss w.r.t y_pred\n",
    "        dy_dz = sigmoid_derivative(z)  # Derivative of y_pred w.r.t z\n",
    "        dz_dw = x  # Derivative of z w.r.t w\n",
    "        dz_db = 1  # Derivative of z w.r.t b\n",
    "\n",
    "        # Compute gradients for weight and bias\n",
    "        dL_dw = dL_dy * dy_dz * dz_dw  # Gradient of loss w.r.t w\n",
    "        dL_db = dL_dy * dy_dz * dz_db  # Gradient of loss w.r.t b\n",
    "\n",
    "        # Step 9: Update parameters using gradient descent\n",
    "        w -= learning_rate * dL_dw  # Update weight\n",
    "        b -= learning_rate * dL_db  # Update bias\n",
    "\n",
    "    # Step 10: Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 11: Print final trained parameters\n",
    "print(\"Final weight:\", w)\n",
    "print(\"Final bias:\", b)\n",
    "\n",
    "# Step 12: Test the trained neuron with the same input values\n",
    "output = [sigmoid(w * x + b) for x in X_train]\n",
    "print(\"Final Neuron Outputs:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Analysis of the Output\n",
    "\n",
    "‚úÖ Loss Decreasing: The loss started at **1.0911** and reduced to **0.0176**, showing that the neuron optimized itself over epochs.\n",
    "\n",
    "‚úÖ **Final Weight** (`w`) and **Bias** (`b`):\n",
    "\n",
    "- `w = 4.68`: This means the neuron gives a **strong weight** to the input.\n",
    "- `b = 2.22`: A positive bias shifts the output towards **1**. ‚úÖ Final Outputs:\n",
    "\n",
    "- **Low values (-2, -1) ‚Üí Close to 0** ‚úÖ\n",
    "- **High values (0, 1, 2) ‚Üí Close to 1** ‚úÖ\n",
    "- The neuron correctly classifies the input data **as expected!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
