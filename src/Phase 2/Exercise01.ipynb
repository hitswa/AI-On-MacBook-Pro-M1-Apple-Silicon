{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Phase 1: Single Neuron Implementation\n",
    "\n",
    "**Concepts to Cover**\n",
    "\n",
    "- **Perceptron Model** â€“ Understanding the fundamental unit of a neural network.\n",
    "- **Activation Functions** â€“ Step, Sigmoid, ReLU, and their importance.\n",
    "- **Forward Propagation** â€“ Calculating weighted sum and applying activation function.\n",
    "- **Gradient Descent** â€“ How a neuron learns by updating weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implement a Single Neuron\n",
    "\n",
    "ðŸ”¹  **Task**\n",
    "\n",
    "- Implement a single neuron (perceptron) in Python using only NumPy.\n",
    "- The neuron should take two inputs and have a trainable weight and bias.\n",
    "- Use the sigmoid activation function.\n",
    "- Implement forward propagation and gradient descent to adjust weights.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Initialize **random weights** and **bias**.\n",
    "- Compute the **weighted sum** of inputs.\n",
    "    Z = W1 . X1 + W2 . X2 + b\n",
    "- Apply the **sigmoid activation function**.\n",
    "    A = 1 / 1 + e^-z\n",
    "- Calculate the loss (Mean Squared Error or **Binary Cross-Entropy**.\n",
    "- Perform **gradient descent** to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9452\n",
      "Epoch 500, Loss: 0.2621\n",
      "Epoch 1000, Loss: 0.1799\n",
      "Epoch 1500, Loss: 0.1417\n",
      "Epoch 2000, Loss: 0.1191\n",
      "Epoch 2500, Loss: 0.1040\n",
      "Epoch 3000, Loss: 0.0932\n",
      "Epoch 3500, Loss: 0.0849\n",
      "Epoch 4000, Loss: 0.0784\n",
      "Epoch 4500, Loss: 0.0731\n",
      "\n",
      "Final Predictions:\n",
      "Input: [0 0], Predicted Output: 0.0009\n",
      "Input: [0 1], Predicted Output: 0.0819\n",
      "Input: [1 0], Predicted Output: 0.0819\n",
      "Input: [1 1], Predicted Output: 0.9022\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function (used for binary classification)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # Applies sigmoid activation to input x\n",
    "\n",
    "# Derivative of sigmoid function (used for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Computes the derivative of the sigmoid function\n",
    "\n",
    "# Binary cross-entropy loss function (measures the performance of classification model)\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels (0 or 1)\n",
    "    y_pred (numpy array): Predicted probabilities (between 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "    float: Mean binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Training data for the AND logic gate\n",
    "X = np.array([[0, 0],  # Input combination 1\n",
    "              [0, 1],  # Input combination 2\n",
    "              [1, 0],  # Input combination 3\n",
    "              [1, 1]]) # Input combination 4\n",
    "\n",
    "y = np.array([[0],  # AND output for (0,0)\n",
    "              [0],  # AND output for (0,1)\n",
    "              [0],  # AND output for (1,0)\n",
    "              [1]]) # AND output for (1,1)\n",
    "\n",
    "# Initialize weights and bias randomly\n",
    "np.random.seed(42)  # Ensures reproducibility\n",
    "weights = np.random.randn(2, 1)  # 2 input features â†’ 1 output neuron\n",
    "bias = np.random.randn(1)  # Single bias value\n",
    "\n",
    "# Set learning rate\n",
    "lr = 0.1  \n",
    "epochs = 5000  # Number of iterations for training\n",
    "\n",
    "# Training loop (Gradient Descent Optimization)\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    weighted_sum = np.dot(X, weights) + bias  # Compute weighted sum of inputs\n",
    "    output = sigmoid(weighted_sum)  # Apply activation function to get predictions\n",
    "    \n",
    "    # Compute loss (Binary Cross-Entropy)\n",
    "    loss = binary_cross_entropy(y, output)\n",
    "    \n",
    "    # Backpropagation (Calculate Gradients)\n",
    "    error = output - y  # Compute error (difference between prediction and actual value)\n",
    "    d_output = error * sigmoid_derivative(output)  # Compute gradient of loss w.r.t. output\n",
    "    \n",
    "    # Update weights and bias using gradient descent\n",
    "    weights -= lr * np.dot(X.T, d_output)  # Adjust weights based on gradient\n",
    "    bias -= lr * np.sum(d_output)  # Adjust bias based on gradient\n",
    "\n",
    "    # Print loss every 500 epochs for monitoring\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final predictions after training\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for i, x in enumerate(X):\n",
    "    pred = sigmoid(np.dot(x, weights) + bias)  # Compute final prediction for each input\n",
    "    print(f\"Input: {x}, Predicted Output: {pred[0]:.4f}\")  # Print input and corresponding prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Significance of This Exercise\n",
    "\n",
    "1. **Understanding a Single Neuron** â€“ This is the fundamental building block of deep learning.\n",
    "2. **Activation Functions** â€“ Using sigmoid to make predictions between 0 and 1.\n",
    "3. **Forward Propagation** â€“ Computing the weighted sum and activation.\n",
    "4. **Loss Function** â€“ Binary Cross-Entropy measures how well the neuron performs.\n",
    "5. **Gradient Descent** â€“ Updating weights using derivatives to minimize loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
