{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ Phase 3: Implementing a Neural Network in TensorFlow & PyTorch\n",
    "\n",
    "**Concepts to Cover**\n",
    "\n",
    "- **Why use TensorFlow & PyTorch?** ‚Äì Higher-level APIs for defining models.\n",
    "- **Defining Layers** ‚Äì Using tf.keras (TensorFlow) and torch.nn (PyTorch).\n",
    "- **Training Loops** ‚Äì Using built-in optimizers & loss functions.\n",
    "- **Batch Processing** ‚Äì Efficient training using mini-batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Exercise 3: Implement the same 3-layer XOR network in TensorFlow & PyTorch\n",
    "\n",
    "**üîπ Task**\n",
    "\n",
    "- Implement a **3-layer neural network** using:\n",
    "    - TensorFlow (`tf.keras`)\n",
    "    - PyTorch (`torch.nn.Module`)\n",
    "- Train on the **XOR dataset**.\n",
    "- Use the **Binary Cross-Entropy Loss (`BCE`)**.\n",
    "- Compare both frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 19:42:05.896800: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-03-15 19:42:05.896834: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-03-15 19:42:05.896843: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-03-15 19:42:05.896862: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-15 19:42:05.896874: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-03-15 19:42:07.305021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\n",
      "TensorFlow Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# XOR dataset (logical XOR function)\n",
    "X = np.array([[0, 0],  # Input: (0,0)\n",
    "              [0, 1],  # Input: (0,1)\n",
    "              [1, 0],  # Input: (1,0)\n",
    "              [1, 1]], # Input: (1,1)\n",
    "             dtype=np.float32)  # Data type: float32 for TensorFlow compatibility\n",
    "\n",
    "y = np.array([[0],  # Expected XOR output for (0,0)\n",
    "              [1],  # Expected XOR output for (0,1)\n",
    "              [1],  # Expected XOR output for (1,0)\n",
    "              [0]], # Expected XOR output for (1,1)\n",
    "             dtype=np.float32)  # Labels must match the input data type\n",
    "\n",
    "# Define the neural network model using Keras Sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),  # Explicit input layer with 2 input features\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),  # Hidden Layer: 4 neurons, sigmoid activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')   # Output Layer: 1 neuron, sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer, loss function, and evaluation metric\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),  # Adam optimizer with LR=0.1\n",
    "              loss='binary_crossentropy',  # Binary Cross-Entropy for classification\n",
    "              metrics=['accuracy'])  # Track accuracy during training\n",
    "\n",
    "# Train the model on the XOR dataset\n",
    "model.fit(X, y, epochs=5000, verbose=0)  # Train silently for 5000 epochs\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Display final predictions after training\n",
    "print(\"\\nTensorFlow Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {p[0]:.4f}\")  # Print formatted predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in TensorFlow (Optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU Available! Training on GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model trained and saved as 'xor_model.h5'.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\n",
      "Optimized TensorFlow Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0000\n",
      "\n",
      "üìä To visualize training logs, run:\n",
      "   tensorboard --logdir=logs/xor_model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)  # Input features\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)  # Expected XOR outputs\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"‚úÖ GPU Available! Training on GPU...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, training on CPU...\")\n",
    "\n",
    "# Define the optimized neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),  # Explicit Input Layer with 2 input features\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),  # Hidden Layer: 4 neurons, sigmoid activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output Layer: 1 neuron, sigmoid activation\n",
    "])\n",
    "\n",
    "# Compile with optimized settings\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.1),  # Faster optimizer (RMSprop)\n",
    "    loss='binary_crossentropy',  # Loss function for binary classification\n",
    "    metrics=['accuracy']  # Tracking accuracy during training\n",
    ")\n",
    "\n",
    "# Setup callbacks: Early stopping & TensorBoard logging\n",
    "log_dir = \"logs/xor_model\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True),  # Stop if no improvement\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)  # TensorBoard for visualization\n",
    "]\n",
    "\n",
    "# Train with mini-batch gradient descent (batch_size=2)\n",
    "with tf.device('/GPU:0' if gpus else '/CPU:0'):\n",
    "    history = model.fit(X, y, epochs=1000, batch_size=2, verbose=0, callbacks=callbacks)  # Silent training\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"xor_model.h5\")\n",
    "print(\"\\n‚úÖ Model trained and saved as 'xor_model.h5'.\")\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"xor_model.h5\")\n",
    "\n",
    "# Final Predictions using the loaded model\n",
    "predictions = loaded_model.predict(X)\n",
    "print(\"\\nOptimized TensorFlow Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {p[0]:.4f}\")\n",
    "\n",
    "# Instructions for TensorBoard usage\n",
    "print(\"\\nüìä To visualize training logs, run:\")\n",
    "print(\"   tensorboard --logdir=logs/xor_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ Optimizations Applied\n",
    "\n",
    "1. ‚úÖ Reduced Epochs: Down from 5000 to 1000 (should still converge).\n",
    "2. ‚úÖ Used RMSprop Optimizer: Faster than Adam for small datasets.\n",
    "3. ‚úÖ Batch Training (batch_size=2): Instead of feeding the entire dataset at once.\n",
    "4. ‚úÖ Forced GPU Usage: Uses with tf.device('/GPU:0') if available.\n",
    "\n",
    "This should significantly speed up training while keeping accuracy high. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Predictions:\n",
      "Input: [0. 0.], Predicted Output: 0.0000\n",
      "Input: [0. 1.], Predicted Output: 1.0000\n",
      "Input: [1. 0.], Predicted Output: 1.0000\n",
      "Input: [1. 1.], Predicted Output: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR dataset (Input features and corresponding target labels)\n",
    "X_torch = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # Input: 2 features\n",
    "y_torch = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)  # Output: XOR results\n",
    "\n",
    "# Define the Neural Network Model\n",
    "class XORNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNeuralNet, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # Hidden Layer with 4 neurons\n",
    "        self.output = nn.Linear(4, 1)  # Output Layer with 1 neuron\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden(x))  # Activation function for hidden layer\n",
    "        x = torch.sigmoid(self.output(x))  # Activation function for output layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = XORNeuralNet()\n",
    "\n",
    "# Define the loss function (Binary Cross-Entropy since it's a binary classification problem)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Use Adam optimizer with a learning rate of 0.1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop for 5000 epochs\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients before each step\n",
    "    outputs = model(X_torch)  # Forward pass: Compute predictions\n",
    "    loss = criterion(outputs, y_torch)  # Compute loss\n",
    "    loss.backward()  # Backpropagation: Compute gradients\n",
    "    optimizer.step()  # Update weights based on computed gradients\n",
    "\n",
    "# Final Predictions (Inference)\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    predictions = model(X_torch)\n",
    "\n",
    "# Print the final predictions\n",
    "print(\"\\nPyTorch Predictions:\")\n",
    "for i, p in enumerate(predictions):\n",
    "    print(f\"Input: {X_torch[i].numpy()}, Predicted Output: {p.item():.4f}\")  # Convert tensors to NumPy for display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Significance of This Exercise\n",
    "\n",
    "- **TensorFlow vs PyTorch** ‚Äì You now see how both frameworks implement the same logic.\n",
    "- **Higher-Level Abstraction** ‚Äì TensorFlow provides `Sequential()`, while PyTorch gives more control.\n",
    "- **Backpropagation Handling** ‚Äì PyTorch requires `loss.backward()`, while TensorFlow abstracts it away.\n",
    "- **Optimization** ‚Äì Adam optimizer helps in faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
