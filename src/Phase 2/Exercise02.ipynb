{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Phase 2: Multilayer Perceptron (MLP)\n",
    "\n",
    "**Concepts to Cover**\n",
    "\n",
    "- **Multilayer Perceptron (MLP)** â€“ Extending from a single neuron to multiple layers.\n",
    "- **Hidden Layers** â€“ Why deep networks are more powerful.\n",
    "- **Backpropagation** â€“ How gradients flow through layers to update weights.\n",
    "- **Activation Functions** â€“ Sigmoid, ReLU, and their impact.\n",
    "- **Training a Neural Network** â€“ Using forward propagation and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Implement a 3-Layer Neural Network (Only Using NumPy)\n",
    "\n",
    "**ðŸ”¹ Task**\n",
    "\n",
    "- Implement a **3-layer neural network** (Input â†’ Hidden â†’ Output) in **pure NumPy**.\n",
    "- Train it to learn the **XOR function** (which a single perceptron cannot solve!).\n",
    "- Use the **sigmoid activation** for the hidden and output layers.\n",
    "- Implement **forward propagation** and **backpropagation** to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.7953\n",
      "Epoch 1000, Loss: 0.6236\n",
      "Epoch 2000, Loss: 0.3028\n",
      "Epoch 3000, Loss: 0.1412\n",
      "Epoch 4000, Loss: 0.0949\n",
      "Epoch 5000, Loss: 0.0740\n",
      "Epoch 6000, Loss: 0.0620\n",
      "Epoch 7000, Loss: 0.0540\n",
      "Epoch 8000, Loss: 0.0483\n",
      "Epoch 9000, Loss: 0.0440\n",
      "\n",
      "Final Predictions:\n",
      "Input: [0 0], Predicted Output: 0.0335\n",
      "Input: [0 1], Predicted Output: 0.9593\n",
      "Input: [1 0], Predicted Output: 0.9607\n",
      "Input: [1 1], Predicted Output: 0.0456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function (used to introduce non-linearity)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # Computes sigmoid activation\n",
    "\n",
    "# Derivative of the sigmoid function (used for backpropagation)\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)  # Computes the derivative of sigmoid\n",
    "\n",
    "# XOR training data (XOR is a non-linearly separable problem)\n",
    "X = np.array([[0, 0],  # Input: (0,0)\n",
    "              [0, 1],  # Input: (0,1)\n",
    "              [1, 0],  # Input: (1,0)\n",
    "              [1, 1]]) # Input: (1,1)\n",
    "\n",
    "y = np.array([[0],  # Expected XOR output for (0,0)\n",
    "              [1],  # Expected XOR output for (0,1)\n",
    "              [1],  # Expected XOR output for (1,0)\n",
    "              [0]]) # Expected XOR output for (1,1)\n",
    "\n",
    "# Initialize neural network parameters\n",
    "np.random.seed(42)  # Ensures reproducibility\n",
    "input_size = 2  # Number of input features\n",
    "hidden_size = 4  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Number of neurons in the output layer\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights from Input Layer â†’ Hidden Layer\n",
    "b1 = np.random.randn(hidden_size)  # Bias for Hidden Layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weights from Hidden Layer â†’ Output Layer\n",
    "b2 = np.random.randn(output_size)  # Bias for Output Layer\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 0.1  # Learning rate (controls step size for weight updates)\n",
    "epochs = 10000  # Number of training iterations\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward Propagation (Feedforward Pass)\n",
    "    Z1 = np.dot(X, W1) + b1  # Compute weighted sum for the hidden layer\n",
    "    A1 = sigmoid(Z1)  # Apply activation function (sigmoid) in hidden layer\n",
    "\n",
    "    Z2 = np.dot(A1, W2) + b2  # Compute weighted sum for the output layer\n",
    "    A2 = sigmoid(Z2)  # Apply activation function (sigmoid) in output layer\n",
    "\n",
    "    # Compute Loss (Binary Cross-Entropy)\n",
    "    loss = -np.mean(y * np.log(A2) + (1 - y) * np.log(1 - A2))  # Loss function\n",
    "\n",
    "    # Backpropagation (Error Propagation)\n",
    "    error_output = A2 - y  # Compute error at the output layer\n",
    "    d_output = error_output * sigmoid_derivative(A2)  # Compute gradient for output layer\n",
    "\n",
    "    error_hidden = np.dot(d_output, W2.T)  # Compute error at the hidden layer\n",
    "    d_hidden = error_hidden * sigmoid_derivative(A1)  # Compute gradient for hidden layer\n",
    "\n",
    "    # Update Weights and Biases using Gradient Descent\n",
    "    W2 -= lr * np.dot(A1.T, d_output)  # Adjust weights for Hidden â†’ Output\n",
    "    b2 -= lr * np.sum(d_output, axis=0)  # Adjust bias for Output Layer\n",
    "\n",
    "    W1 -= lr * np.dot(X.T, d_hidden)  # Adjust weights for Input â†’ Hidden\n",
    "    b1 -= lr * np.sum(d_hidden, axis=0)  # Adjust bias for Hidden Layer\n",
    "\n",
    "    # Print loss every 1000 epochs for tracking progress\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Final Predictions after training\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for i, x in enumerate(X):\n",
    "    hidden_layer = sigmoid(np.dot(x, W1) + b1)  # Forward pass for hidden layer\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, W2) + b2)  # Forward pass for output layer\n",
    "    print(f\"Input: {x}, Predicted Output: {output_layer[0]:.4f}\")  # Display results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Significance of This Exercise\n",
    "\n",
    "- **From Single Neuron to Multi-Layer Perceptron (MLP)** â€“ You now understand why adding hidden layers makes neural networks more powerful.\n",
    "- **Activation Functions Matter** â€“ The sigmoid function enables non-linearity, allowing the network to solve XOR.\n",
    "- **Backpropagation** â€“ Learning happens by updating weights using gradient descent.\n",
    "- **Why Deep Learning Works** â€“ This is the foundation of all deep networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning-env)",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
